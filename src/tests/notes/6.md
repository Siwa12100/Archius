# Partie 5 : Validation et analyse des résultats

[...retour en arriere](../menu.md)

## 1. Phases de validation
**Validation des spécifications**
- Objectif : s'assurer que les spécifications répondent aux besoins exprimés par le client.
- Méthodes :
  - **Revue de spécifications** : vérification manuelle des documents pour détecter des incohérences ou des ambiguïtés.
  - **Prototypage** : créer un modèle fonctionnel ou visuel pour confirmer que les spécifications sont comprises.
  - **Formalisation** : utiliser des langages formels (UML, BPMN, etc.) pour définir précisément les exigences.
- Exemple : valider un diagramme de cas d’utilisation pour vérifier que toutes les fonctionnalités demandées sont bien représentées.

**Vérification de l’implantation (phases de test)**
- Consiste à valider que l’implantation (le code) respecte les spécifications définies.
- Phases typiques :
  1. **Tests unitaires** : valider chaque composant de manière isolée.
  2. **Tests d’intégration** : vérifier l’interaction entre les modules.
  3. **Tests système** : valider l’ensemble du système dans un environnement simulé ou réel.
  4. **Tests d’acceptation** : réalisés avec ou par le client pour valider les fonctionnalités clés.
- Exemple : exécuter des tests d’intégration pour valider que l’API et la base de données communiquent correctement.

---

## 2. Analyse des résultats
**Verdicts : PASS, FAIL, INCONCLUSIVE**
- **PASS** : le test est validé, le comportement attendu est observé.
- **FAIL** : le test échoue, une déviation par rapport au comportement attendu est détectée.
  - Nécessite une analyse approfondie pour identifier la cause (erreur de code, de test, ou environnement).
- **INCONCLUSIVE** : le test ne fournit pas de résultat clair (problème d’environnement, dépendances manquantes, etc.).
  - Demande une reprise ou une modification des conditions d’exécution.

**Gestion des tests échoués et non conclusifs**
- **Tests échoués** :
  - Analyser les logs d’exécution pour identifier l’origine de l’échec.
  - Catégoriser l’échec :
    - **Erreur fonctionnelle** : non-respect des spécifications.
    - **Erreur environnementale** : problème de configuration ou de ressources.
  - Planifier la correction et ré-exécuter le test après correction.
- **Tests non conclusifs** :
  - Revoir les prérequis du test (données, environnement, dépendances).
  - Identifier et éliminer les obstacles techniques (problèmes de connexion, erreurs réseau, etc.).

---

## 3. Documentation
**Rapport de tests**
- Contient un résumé des tests exécutés, leurs résultats, et les éventuelles anomalies détectées.
- Structure typique :
  1. **Résumé exécutif** : vue globale des résultats (nombre de tests réussis, échoués, etc.).
  2. **Détails des tests** : description des cas de tests, données utilisées, résultats obtenus.
  3. **Anomalies** : liste des problèmes identifiés, leur priorité, et leur impact.
  4. **Recommandations** : suggestions pour améliorer la qualité ou corriger les anomalies.

**Suivi des anomalies et corrections**
- **Enregistrement des anomalies** : consigner chaque problème détecté avec des détails (type, emplacement, gravité).
  - Utilisation d’outils comme JIRA ou Bugzilla pour centraliser et suivre les anomalies.
- **Priorisation des corrections** :
  - Basée sur la gravité et l’impact sur les fonctionnalités critiques.
  - Exemple : corriger une faille de sécurité avant une anomalie mineure d’interface utilisateur.
- **Suivi des corrections** :
  - Assurer que chaque anomalie corrigée est retestée.
  - Utiliser des **tests de non-régression** pour vérifier que les corrections n’introduisent pas de nouveaux problèmes.

---

[...retour en arriere](../menu.md)